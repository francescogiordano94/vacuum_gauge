{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta = pd.read_csv('df_delta.csv') # cointains labels for delta VG\n",
    "df_ok = pd.read_csv('df_ok.csv')  #contains labels for ok VG\n",
    "\n",
    "df_raw = pd.read_csv('df_raw.csv') ## contains full reading of each VG\n",
    "df_raw = df_raw.set_index(['gauge_id','fillNumber'], drop=True)\n",
    "\n",
    "df_labels = pd.concat([df_ok, df_delta], sort=False, axis=0)\n",
    "\n",
    "\n",
    "df_VG = pd.merge(df_raw, df_labels, on =['gauge_id','fillNumber'])\n",
    "df_VG = df_VG.set_index(['gauge_id','fillNumber'], drop=True)\n",
    "\n",
    "## Removing categorical values\n",
    "df_VG.y.replace(to_replace=['ok', 'delta'], value=[0, 1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "class RowScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, scaling_method='Standard'):\n",
    "        self.scaling_options = ['Standard', 'MinMax', 'MaxAbs', 'Robust']\n",
    "        assert (scaling_method in self.scaling_options), 'scaling_method:' + scaling_method + ' not in ' + str(self.scaling_options)\n",
    "        self.scaling_method = scaling_method\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.scaling_method == 'Robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaling_method == 'MinMax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif self.scaling_method == 'Standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaling_method == 'MaxAbs':\n",
    "            self.scaler = MaxAbsScaler()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.scaler.fit_transform(X.transpose()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(6045, 3000)\n(186, 3000)\n(92, 3000)\n(186,)\n(92,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_unlabeled = df_raw.drop(index=df_VG.index)\n",
    "\n",
    "X_train_ae = np.array(df_unlabeled)\n",
    "X = np.array(df_VG.iloc[:, :-1])\n",
    "y = np.array(df_VG.iloc[:, -1])\n",
    "\n",
    "scaler = RowScaler()\n",
    "\n",
    "X_train_ae = scaler.fit_transform(X_train_ae)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train_ae.shape)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./data/models/autoencoder/stacked_pca_03_dim.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsequential_2 (Sequential)    (None, 3)                 9012003   \n_________________________________________________________________\nsequential_3 (Sequential)    (None, 3000)              12012     \n=================================================================\nTotal params: 9,024,015\nTrainable params: 9,024,015\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version ='0001'\n",
    "model_name = 'stacked_pca_ndim_003'\n",
    "model_path = os.path.join(model_name, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /Users/Francesco/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: stacked_pca_ndim_003/0001/assets\n"
    }
   ],
   "source": [
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.saved_model.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = saved_model(tf.constant(X_train[[0]], dtype=tf.float32), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 3000), dtype=float32, numpy=\narray([[-0.77362126, -0.78488976, -0.7863362 , ..., -0.41150382,\n        -0.4224767 , -0.39437792]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2020-05-04 12:45:53.195029: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2020-05-04 12:45:53.212224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd2bbc22370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-05-04 12:45:53.212255: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\nWARNING:tensorflow:From /Users/Francesco/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow_core/python/tools/saved_model_cli.py:420: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\nResult for output key sequential_3:\n[[-0.77362144 -0.7848898  -0.78633636 ... -0.41150385 -0.42247677\n  -0.39437795]\n [-1.5438116  -1.5244832  -1.5194207  ... -0.75834846 -0.7846668\n  -0.7960645 ]\n [-0.8461399  -0.8194741  -0.8274206  ... -0.7770494  -0.7977637\n  -0.807449  ]\n ...\n [-1.8295584  -1.8179966  -1.8071463  ... -0.7127359  -0.7401085\n  -0.74728304]\n [-0.8715365  -0.8566154  -0.85428166 ... -0.56015295 -0.58003867\n  -0.5830262 ]\n [-1.6388555  -1.6226051  -1.6162084  ... -0.74839276 -0.774789\n  -0.78389513]]\n"
    }
   ],
   "source": [
    "!saved_model_cli run --dir stacked_pca_ndim_003/0001 --tag_set serve \\\n",
    "                     --signature_def serving_default \\\n",
    "                     --inputs sequential_2_input=test.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker pull tensorflow/serving\n",
    "\n",
    "export ML_PATH='/Users/Francesco/cernbox/SWAN_projects/VacuumGauge/vacuum_gauge'\n",
    "docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
    "   -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n",
    "   -e MODEL_NAME=my_mnist_model \\\n",
    "   tensorflow/serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test.npy', X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}